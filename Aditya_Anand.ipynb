{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandaditya07/Smart-Energy-Consumption-Analysis-and-Prediction-using-Machine-Learning-with-Device-Level-Insights/blob/main/Aditya_Anand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQeSVyk90doJ"
      },
      "source": [
        " **WEEK 1 & 2**\n",
        "\n",
        "\n",
        "**Module 1: Data Collection and Understanding**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlDn1SCEqrAw"
      },
      "source": [
        "1. **Define project scope and functional objectives for smart energy analysis.**\n",
        "\n",
        "\n",
        "\n",
        "This project is about understanding how much electricity different appliances in a smart home use. Instead of only seeing one total electricity bill at the end of the month, we want to see which device uses how much power and when. This will help us know where energy is being wasted.\n",
        "\n",
        "**Functional Objectives**\n",
        "\n",
        "*   Track energy usage of each device and each room separately.\n",
        "*   Show energy use in the form of graphs (hourly, daily, weekly).\n",
        "*   Find which devices use the most power and at what time.\n",
        "*   Use machine learning to predict future electricity use.\n",
        "*   Help save electricity by giving suggestions to reduce unnecessary usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txZKN2EmtDNe"
      },
      "source": [
        "\n",
        "2. **Collect and structure the SmartHome Energy Monitoring Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tV1xxThcmdQL"
      },
      "outputs": [],
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_1b7V72nu8A",
        "outputId": "98fe3955-1ade-4800-bf82-ffa573a72df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "path = \"/content/drive/MyDrive/HomeC_augmented.csv\"\n",
        "df = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3yQCNdTcsopC"
      },
      "outputs": [],
      "source": [
        "# Read the CSV\n",
        "df_raw = pd.read_csv(path)\n",
        "\n",
        "print(\"Original shape:\", df_raw.shape)\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vxPaei2oXwl"
      },
      "outputs": [],
      "source": [
        "# Column names\n",
        "df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDq0B2ssod_T"
      },
      "outputs": [],
      "source": [
        "# Basic info – data types, nulls, etc.\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifAnuccqoh87"
      },
      "outputs": [],
      "source": [
        "# Basic statistics for numerical columns\n",
        "df.describe().T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D5fZeK1X1dgW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Change 'timestamp' to the actual time column name from df_raw.columns\n",
        "time_col = \"time\"   # e.g. \"date\", \"time\", \"Datetime\" etc.\n",
        "\n",
        "# Convert to datetime\n",
        "df_raw[time_col] = pd.to_datetime(df_raw[time_col], errors='coerce')\n",
        "\n",
        "# Drop rows where timestamp could not be parsed\n",
        "df_raw = df_raw.dropna(subset=[time_col])\n",
        "\n",
        "# Sort by time\n",
        "df_raw = df_raw.sort_values(time_col);\n",
        "\n",
        "# Set timestamp as index\n",
        "df = df_raw.set_index(time_col)\n",
        "\n",
        "print(\"After setting time index:\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27tVQcysz9wq"
      },
      "outputs": [],
      "source": [
        "# All devices/measurements (since time is now index)\n",
        "device_cols = df.columns.tolist()\n",
        "print(\"Device / sensor columns:\", device_cols[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FID5rcjC1rzZ"
      },
      "source": [
        "\n",
        "\n",
        "3. **Verify data integrity, handle missing timestamps, and perform exploratory analysis.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2wHaS2f2tRZ"
      },
      "source": [
        "\n",
        "\n",
        "i. Check Data Integrity\n",
        "\n",
        "  We verify whether the dataset has:\n",
        "\n",
        "*   Repeated timestamps\n",
        "*   Empty/Missing data\n",
        "\n",
        "    If yes, we fix them.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zZABdJ33LkU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "print(\"\\n~~~~~~~ MISSING TIMESTAMP HANDLING ~~~~~~~\")\n",
        "# Try to guess the time gap between readings (like 1 hour / 5 min)\n",
        "inferred_freq = pd.infer_freq(df.index[:100])\n",
        "print(\"Inferred frequency:\", inferred_freq)\n",
        "\n",
        "# If frequency cannot be detected → assume 1 hour gap\n",
        "if inferred_freq is None:\n",
        "    inferred_freq = '1H'\n",
        "\n",
        "# Create a new continuous timeline with no gaps\n",
        "full_range = pd.date_range(start=df.index.min(),\n",
        "                           end=df.index.max(),\n",
        "                           freq=inferred_freq)\n",
        "\n",
        "# Reindex so dataset follows this timeline\n",
        "df = df.reindex(full_range)\n",
        "df.index.name = \"timestamp\"\n",
        "\n",
        "# Fill empty values created by reindexing\n",
        "df = df.ffill().bfill()\n",
        "print(\"Missing values after filling:\")\n",
        "print(df.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpl-bss53p4D"
      },
      "source": [
        "ii. Handle Missing Timestamps\n",
        "\n",
        "We ensure time moves smoothly with no missing timestamps,\n",
        "and we fill gaps in the data by copying nearby values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB1cddro35ti"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "print(\"\\n~~~~~~~ MISSING TIMESTAMP HANDLING ~~~~~~~\")\n",
        "# Try to guess the time gap between readings (like 1 hour / 5 min)\n",
        "inferred_freq = pd.infer_freq(df.index[:100])\n",
        "print(\"Inferred frequency:\", inferred_freq)\n",
        "# If frequency cannot be detected → assume 1 hour gap\n",
        "if inferred_freq is None:\n",
        "    inferred_freq = '1H'\n",
        "# Create a new continuous timeline with no gaps\n",
        "full_range = pd.date_range(start=df.index.min(),\n",
        "                           end=df.index.max(),\n",
        "                           freq=inferred_freq)\n",
        "# Reindex so dataset follows this timeline\n",
        "df = df.reindex(full_range)\n",
        "df.index.name = \"timestamp\"\n",
        "# Fill empty values created by reindexing\n",
        "df = df.ffill().bfill()\n",
        "print(\"Missing values after filling:\")\n",
        "print(df.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssyE5kM94IfG"
      },
      "source": [
        "iii. Exploratory Data Analysis\n",
        "* Minimum, maximum, average energy usage per device\n",
        "\n",
        "* Graph that shows how energy usage changes with time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTbPXNFA4dR1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(\"\\n~~~~~~EXPLORATORY ANALYSIS ~~~~~~~\")\n",
        "# Show basic numeric statistics for all device columns\n",
        "display(df.describe().T)\n",
        "# Show how a few device values change over time\n",
        "plt.figure(figsize=(12,4))\n",
        "# Exclude 'Unnamed: 0' from sample_cols for better visualization of energy consumption\n",
        "sample_cols = [col for col in df.select_dtypes(include=[np.number]).columns if col != 'Unnamed: 0'][:3]\n",
        "for col in sample_cols:\n",
        "    plt.plot(df.index, df[col], label=col)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Energy Consumption\")\n",
        "plt.title(\"Sample Energy Consumption Over Time\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vNQ_v3z5NNd"
      },
      "source": [
        "**iv**. **Organize energy readings by device, room, and timestamp.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzoHN_aR5S_-"
      },
      "outputs": [],
      "source": [
        "# Make a copy to be safe\n",
        "df_device = df.copy()\n",
        "# Select all numeric columns as device columns\n",
        "device_cols = df_device.select_dtypes(include=['number']).columns.tolist()\n",
        "print(\"Device / sensor columns:\", device_cols)\n",
        "# Convert from wide format → long format\n",
        "df_long = df_device.reset_index().melt(\n",
        "    id_vars=[\"timestamp\"],       # column that stays fixed (time)\n",
        "    value_vars=device_cols,      # columns that will become 'device'\n",
        "    var_name=\"device\",           # new column name for device name\n",
        "    value_name=\"energy\"          # new column name for energy value\n",
        ")\n",
        "print(\"Long format shape:\", df_long.shape)\n",
        "df_long.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_V-b8Dhd6rOT"
      },
      "outputs": [],
      "source": [
        "# Example device → room mapping\n",
        "# IMPORTANT: change keys to match your real device names\n",
        "room_map = {\n",
        "    \"Kitchen_Light\": \"Kitchen\",\n",
        "    \"Fridge\": \"Kitchen\",\n",
        "    \"AC_Bedroom\": \"Bedroom\",\n",
        "    \"TV_LivingRoom\": \"Living Room\",\n",
        "    # Add more device: room pairs here...\n",
        "}\n",
        "# Create 'room' column using the mapping\n",
        "df_long[\"room\"] = df_long[\"device\"].map(room_map).fillna(\"Unknown\")\n",
        "# Show first few organized rows\n",
        "df_long.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiKCxkVC-prZ"
      },
      "source": [
        "**Module 2: Data Cleaning and Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pb_VewgQ8aV"
      },
      "source": [
        "i. Handle missing values and outliers in power consumption readings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZHROi3AtJtX"
      },
      "outputs": [],
      "source": [
        "import numpy as np # Ensure numpy is imported for np.number, if not already\n",
        "# Missing values check\n",
        "print(\"Missing values before cleaning:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "# Fill missing values using forward & backward fill\n",
        "df = df.ffill().bfill()\n",
        "print(\"Missing values after filling:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "# Remove outliers using 1st and 99th percentile for each numeric column\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "for col in num_cols:\n",
        "    low, high = df[col].quantile([0.01, 0.99])\n",
        "    df[col] = df[col].clip(lower=low, upper=high)\n",
        "\n",
        "print(\"Outliers handled successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkcNWpSAu0Hs"
      },
      "source": [
        "ii. Convert timestamps to datetime format and resample data (hourly/daily)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erBrr3UQu4HR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# PART 2: RESAMPLE DATA (HOURLY / DAILY)\n",
        "\n",
        "# Select only numeric columns for resampling\n",
        "numeric_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Hourly average consumption\n",
        "df_hourly = numeric_df.resample('h').mean()\n",
        "\n",
        "print(\"Hourly data shape:\", df_hourly.shape)\n",
        "df_hourly.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gopnpgdbvbmx"
      },
      "source": [
        "iii. Normalize or scale energy values for model compatibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "briLR00uvgk2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# PART 3: NORMALIZATION / SCALING\n",
        "\n",
        "# Select target and features later\n",
        "df_scaled = df_hourly.copy()\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled[df_hourly.columns] = scaler.fit_transform(df_hourly)\n",
        "\n",
        "df_scaled.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCTSrojwv-hr"
      },
      "source": [
        "iv. Split dataset into training, validation, and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ebxv9yTwBVI"
      },
      "outputs": [],
      "source": [
        "# PART 4: TRAIN / VALIDATION / TEST SPLIT\n",
        "\n",
        "# Select the main target column (CHANGE to your main power column)\n",
        "target_col = df_scaled.columns[0]  # example: first numeric col\n",
        "print(\"Using target:\", target_col)\n",
        "\n",
        "# Create X and y\n",
        "X = df_scaled.drop(columns=[target_col])\n",
        "y = df_scaled[target_col]\n",
        "\n",
        "# Time-based splitting\n",
        "train_size = int(len(df_scaled) * 0.7)\n",
        "val_size = int(len(df_scaled) * 0.15)\n",
        "\n",
        "X_train = X.iloc[:train_size]\n",
        "y_train = y.iloc[:train_size]\n",
        "\n",
        "X_val = X.iloc[train_size:train_size + val_size]\n",
        "y_val = y.iloc[train_size:train_size + val_size]\n",
        "\n",
        "X_test = X.iloc[train_size + val_size:]\n",
        "y_test = y.iloc[train_size + val_size:]\n",
        "\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Validation size:\", len(X_val))\n",
        "print(\"Test size:\", len(X_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kquI_nyrkYp1"
      },
      "source": [
        "**Milestone 2: Week 3-4**\n",
        "\n",
        "\n",
        "Module 3: Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBJ0LaPWk8oA"
      },
      "source": [
        "i. Extract relevant time-based features (hour, day, week, month trends)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FabFqGqKlPNP"
      },
      "outputs": [],
      "source": [
        "# PART 1: TIME-BASED FEATURES\n",
        "df_features = df_scaled.copy()\n",
        "\n",
        "df_features[\"hour\"] = df_features.index.hour\n",
        "df_features[\"dayofweek\"] = df_features.index.dayofweek   # 0=Monday\n",
        "df_features[\"month\"] = df_features.index.month\n",
        "\n",
        "print(\"Time-based features added.\")\n",
        "df_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDtIfA_ClAip"
      },
      "source": [
        "ii. Aggregate device-level consumption statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3mxD6gblQQp"
      },
      "outputs": [],
      "source": [
        "# PART 2: AGGREGATE DEVICE CONSUMPTION\n",
        "df_features[\"total_energy\"] = df_features.select_dtypes(include='number').sum(axis=1)\n",
        "df_features.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZHgPkvdlEQu"
      },
      "source": [
        "iii. Create lag features and moving averages for time series learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I_7IVBglR9b"
      },
      "outputs": [],
      "source": [
        "# PART 3: LAG AND MOVING AVERAGE FEATURES\n",
        "target_col = df_features.columns[0]  # change if needed\n",
        "print(\"Target column:\", target_col)\n",
        "\n",
        "# Lag features (previous values)\n",
        "for lag in [1, 6, 12, 24]:\n",
        "    df_features[f\"{target_col}_lag_{lag}\"] = df_features[target_col].shift(lag)\n",
        "\n",
        "# Rolling/Moving averages\n",
        "df_features[\"rolling_mean_6\"] = df_features[target_col].rolling(6).mean()\n",
        "df_features[\"rolling_mean_12\"] = df_features[target_col].rolling(12).mean()\n",
        "df_features[\"rolling_mean_24\"] = df_features[target_col].rolling(24).mean()\n",
        "\n",
        "# Drop rows created with NaN from shifting\n",
        "df_features = df_features.dropna()\n",
        "\n",
        "df_features.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAq_ybo9lJdp"
      },
      "source": [
        "iv. Prepare final feature set for ML model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQyBY11alUsu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# PART 4: FINAL ML FEATURE MATRIX\n",
        "X = df_features.drop(columns=[target_col])\n",
        "y = df_features[target_col]\n",
        "\n",
        "# Time-based splitting for model training\n",
        "train_size = int(len(df_features) * 0.7)\n",
        "val_size = int(len(df_features) * 0.15)\n",
        "\n",
        "X_train = X.iloc[:train_size]\n",
        "y_train = y.iloc[:train_size]\n",
        "\n",
        "X_val = X.iloc[train_size:train_size+val_size]\n",
        "y_val = y.iloc[train_size:train_size+val_size]\n",
        "\n",
        "X_test = X.iloc[train_size+val_size:]\n",
        "y_test = y.iloc[train_size+val_size:]\n",
        "\n",
        "print(\"Training set:\", X_train.shape)\n",
        "print(\"Validation set:\", X_val.shape)\n",
        "print(\"Testing set:\", X_test.shape)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DPNh7yAV_pUM20xD48_l1TGGXqXPJBMi",
      "authorship_tag": "ABX9TyPjFS6vQ5DB1w64KIryUFT+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}